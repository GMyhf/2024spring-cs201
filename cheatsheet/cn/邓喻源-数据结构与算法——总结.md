# 数据结构与算法——总结

邓喻源 数学科学学院

## Part 1：线性表

线性表（$$List$$）的定义：零个或多个数据元素的**有限**序列。

​	线性表的数据集合为{$$a_{1}$$,$$a_{2}$$……$$a_{n}$$}，该序列有唯一的头元素和尾元素，除了头元素外，每个元素都有唯一的前驱元素，除了尾元素外，每个元素都有唯一的后继元素。

​	线性表中的元素属于相同的数据类型，即每个元素所占的空间相同。



框架：
$$
线性表\begin{cases}
顺序存储——顺序表\\
链式存储\begin{cases}
单链表\\
双链表\\
循环链表
\end{cases}
\end{cases}
$$

### 一、顺序表

​	**顺序表**（Sequential List）是一种线性数据结构，使用一段连续的存储空间来存储数据元素。顺序表通常通过数组实现。

##### 特点：

- **随机访问**：可以通过索引快速访问任意元素，时间复杂度为 $$O(1)$$。
- **固定大小**：初始创建时需要确定容量，无法动态扩展。
- **插入和删除操作**：在中间插入或删除元素时需要移动其他元素，时间复杂度为 $$O(n)$$。。

线性表的优缺点：

优点：1、无须为表中元素之间的逻辑关系而增加额外的存储空间；

​	    2、可以快速的存取表中任一位置的元素。

缺点：1、插入和删除操作需要移动大量元素；

​	    2、当线性表长度较大时，难以确定存储空间的容量；

​	    3、造成存储空间的“碎片”。

### 二、链表

#### 1、单链表

​	在链式结构中，除了要存储数据元素的信息外，还要存储它的后继元素的存储地址。

​	因此，为了表示**每个数据元素$$a_{i}$$与其直接后继元素$$a_{i+1}$$之间的逻辑关系，对数据$$a_{i}$$来说，除了存储其本身的信息之外，还需要存储一个指示其直接后继的信息（即直接后继的存储位置）。我们把存储数据元素信息的域称为数据域，把存储直接后继位置的域称为指针域。指针域中存储的信息称做指针或链。这两部分信息组成数据元素$$a_{i}$$的存储映像，称为结点（$$Node$$​）。**

​	我们把链表中第一个结点的存储位置叫做头指针。有时为了方便对对链表进行操作，会在单链表的第一个结点前附设一个节点，称为头结点，此时头指针指向的结点就是头结点。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210207165354972.png#pic_center)

​	空链表，头结点的直接后继为空。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210207165435359.png#pic_center)

#### 2、双链表

​	**双向链表$$(Double$$ $$Linked$$ $$List)$$​是在单链表的每个结点中，再设置一个指向其前驱结点的指针域。**所以在双向链表中的结点都有两个指针域，一个指向直接后继，另一个指向直接前驱。

##### 特点：

- **双向访问**：可以从任一节点双向遍历链表。
- **动态大小**：可以根据需要动态分配和释放内存。
- **插入和删除操作**：时间复杂度为 O(1)O(1)O(1)。

#### 3、循环链表

​	**将单链表中终端节点的指针端由空指针改为指向头结点，就使整个单链表形成一个环，这种头尾相接的单链表称为单循环链表，简称循环链表。**

​	然而这样会导致访问最后一个结点时需要$$O(n)$$的时间，所以我们可以写出**仅设尾指针的循环链表**。

## Part 2：排序

### 一、冒泡排序$$(Bubble$$ $$Sort)$$

​	冒泡排序是最简单的排序算法，它通过不断交换相邻元素以实现正确的排序结果。

​	时间复杂度：$$O(n^2)$$；空间复杂度：$$O(1)$$​​。

​	冒泡排序是一种原地排序算法，无需额外空间，是稳定的。

### 二、选择排序$$(Selection$$ $$Sort)$$

​	选择排序是一种简单且高效的排序算法，其工作原理是从列表的未排序部分反复选择最小(或最大)元素，并将其移动到列表的已排序部分。

​	时间复杂度：$$O(n^2)$$；空间复杂度：$$O(1)$$。

​	选择排序同样是一种原地排序算法，无需额外空间，但它在小的数据集下相对高效，在极端情况下会具有较大的时间复杂度，是不稳定的。

### 三、快速排序$$(Quick$$ $$Sort)$$

​	快速排序是一种基于分治算法的排序算法，它选择一个元素作为基准，并通过将基准放置在已排序数组中的正确位置来围绕所选择的基准对给定数组进行分区。

​	时间复杂度：最好时为$$O(nlogn)$$，最差时为$$O(n^2)$$；空间复杂度：考虑递归堆栈，为$$O(n)$$，不考虑则为$$O(1)$$。

​	快速排序相对更适用于大数据集，在某些极端情况下会显现出较差的时间复杂度，是不稳定的。

### 四、归并排序$$(Merge$$ $$Sort)$$

​	归并排序作为一种排序算法，其原理是将数组划分为更小的子数组，对每个子数组进行排序，然后将排序后的子数组合并在一起，形成最终的排序数组。（和快速排序同样的分治思想）。对于大规模数据集，归并排序可以将数据分为多个小块，分别排序后写回磁盘，最后进行多路归并。这样可以避免占用过多内存，适合在内存有限的情况下处理大数据。

​	时间复杂度：$$O(nlogn)$$；空间复杂度：$$O(n)$$​。

​	归并排序是一种天然的可并行化算法，而且稳定，因此特别适合用来处理大数据集，但它需要额外空间。

### 五、插入排序$$(Insertion$$ $$Sort)$$

​	插入排序是一种基本的排序算法，其思想在于通过先前的已排好序的数组得到目标插入元素的插入位置，从而达到不断排序的目的。

​	时间复杂度：$$O(n^2)$$；空间复杂度：$$O(1)$$。

​	插入排序是一种稳定的原地排序算法。

### 六、希尔排序$$(Shell$$ $$Sort)$$

​	**希尔排序**（Shell Sort）是插入排序的一种优化版本，通过间隔分组进行排序，提高效率。

##### 特点：

- **分组插入排序**：先将数组分成若干子数组分别进行插入排序，然后逐步减小间隔，最终进行一次插入排序。
- **时间复杂度**：依赖于选择的间隔序列，最坏情况下为 $$O(n^2)$$，但通常性能优于直接插入排序。

​	时间复杂度：（最差）$$O(n^2)$$；空间复杂度：$$O(1)$$​。

​	希尔排序的时间复杂度取决于算法的对象，是不稳定的。

### 七、堆排序$$(Heap$$ $$Sort)$$

​	堆排序是一种基于完全二叉树（堆）的排序算法。它通过将待排序的元素构建成一个堆，然后利用堆的性质来实现排序。在堆中，每个结点的值都必须大于等于其子结点的值。

**堆数据结构**：利用二叉堆（Binary Heap）构建一个最大堆或最小堆。

**时间复杂度**：构建堆的时间复杂度为 $$O(n)$$，每次取出堆顶元素并调整堆的时间复杂度为 $$O(log⁡n)$$，整体时间复杂度为 O(nlog⁡n)。

**空间复杂度**：为 $$O(1)$$。

​	堆排序适合处理大型数据集，采取原地排序，但不稳定，因为可能会交换相同元素。

### 八、计数排序$$(Counting$$ $$Sort)$$​

​	计数排序是一种非比较性的整数排序算法。它的基本思想是对于给定的输入序列中的每一个元素$$x$$，确定该序列中值小于$$x$$的元素的个数。利用这个信息，可以直接确定$$x$$​在输出序列中的位置，从而实现排序。

​	时间复杂度：$$O(n+k)$$，其中$$n$$为序列长度，$$k$$为序列数据范围。空间复杂度也为$$O(n+k)$$。

​	计数排序是一种稳定的排序算法，在数据集数值范围较小的情况下表现优越，但倘若数据离散程度较大则效率将会大大降低。

### 九、桶排序$$(Bucket$$ $$Sort)$$

​	桶排序将数组分割成几个部分（桶），然后对每个桶进行排序，最后将所有桶中的元素合并起来得到最终的有序数组。它的基本思想是将待排序数组分割成若干个较小的数组（桶），每个桶再分别排序，最后按照顺序依次取出各个桶中的元素即可。

​	时间复杂度：理想情况下为$$O(n+k)$$，极端情况下为$$O(n^2)$$。空间复杂度同样。

​	桶排序适合相对均匀稠密的数字序列，在处理小数序列时相对更具有优势（小数更适合用于建立桶的索引与数字之间的映射）。

$$P.S:$$ 桶排序中每个桶使用的排序方法时其它的排序方法，所以桶更适合理解为一种思想，同样的，代码不在此展示。

### 十、基数排序$$(Radix$$ $$Sort)$$

​	**基数排序**（Radix Sort）是一种非比较排序算法，按位数对整数进行排序，从最低位到最高位依次进行。

##### 特点：

- **按位排序**：从最低有效位（LSD）到最高有效位（MSD）依次进行排序。
- **时间复杂度**：为 $$O(d⋅(n+b))$$，其中 ddd 是数字的位数，bbb 是基数。
- **空间复杂度**：为 $$O(n+b)$$。

​	基数排序不依赖于比较操作，适用于整数等固定长度的数据类型，并且在某些情况下具有稳定性。但它需要额外的空间，而且对于位数较大的数据可能不太实用。

## Part 3：栈和队列

​	由于在线性表部分已经基本给出了栈和队列的实现形式，因此在这部分不做赘述。个人认为栈和队列更多地是一种思想，蕴含着元素选取顺序的逻辑。

### 一、栈

**栈**（Stack）是一种线性数据结构，遵循“后进先出”（LIFO, Last In First Out）的原则。栈只允许在一端进行操作，这一端被称为栈顶。插入操作称为“入栈”（push），删除操作称为“出栈”（pop）。

##### 特点

- **操作受限**：只能在栈顶进行插入和删除操作。
- **访问受限**：只能访问栈顶元素，不能直接访问栈底或中间元素。

#### 应用

### 二、队列

**队列**（Queue）是一种线性数据结构，遵循“先进先出”（FIFO, First In First Out）的原则。队列允许在一端进行插入操作（队尾），在另一端进行删除操作（队头）。

##### 特点

- **操作受限**：只能在队尾插入元素，在队头删除元素。
- **访问受限**：只能访问队头元素，不能直接访问队尾或中间元素

## Part 4：树

### 一、定义

#### 1、节点和边

​	**树**由节点及连接节点的边构成。树有以下属性：

​		有一个根节点；
​		除根节点外，其他每个节点都与其唯一的父节点相连；
​		从根节点到其他每个节点都有且仅有一条路径；
​		如果每个节点最多有两个子节点，我们就称这样的树为二叉树。

#### 2、递归

​	一棵树要么为空，要么由一个根节点和零棵或多棵子树构成，子树本身也是一棵树。每棵子树的根节点通过一条边连到父树的根节点。

### 二、数据结构

#### 1. 二叉树（Binary Tree）

二叉堆通过树的特性，由列表实现，每次加入元素时，进行“上浮”操作，而删除元素时则进行“下沉”操作。

- **定义**：每个节点最多有两个子节点，称为左子节点和右子节点。
- **应用**：表达式树、决策树、排序树。

#### 2. 二叉搜索树（Binary Search Tree, BST）

- **定义**：一种二叉树，其中每个节点的左子树的所有节点值小于根节点值，右子树的所有节点值大于根节点值。
- **应用**：动态查找表、实现有序字典。
- **特点**：查找、插入、删除操作的平均时间复杂度为 $$O(log⁡n)$$，最坏情况下为 $$O(n)$$。

#### 3. 平衡二叉搜索树（Balanced Binary Search Tree）

- **定义**：通过旋转操作保持树的高度平衡，使得查找、插入、删除操作的时间复杂度稳定在 $$O(log⁡n)$$。
- 种类：
  - **AVL树**：每个节点的左右子树高度差不超过1。
  - **红黑树**：一种自平衡二叉搜索树，每个节点包含一个颜色位（红色或黑色），通过颜色和旋转操作保持平衡。

#### 4. 完全二叉树（Complete Binary Tree）

- **定义**：每一层节点都是满的，除了最后一层，且最后一层的节点都尽可能靠左排列。
- **应用**：二叉堆的实现。
- **特点**：用数组表示时，父节点和子节点的位置可以通过索引计算。

#### 5. 二叉堆（Binary Heap）

- **定义**：一种完全二叉树，可以是最大堆或最小堆，最大堆中每个节点的值大于等于其子节点的值，最小堆中每个节点的值小于等于其子节点的值。
- **应用**：优先队列、堆排序。
- **特点**：插入和删除操作的时间复杂度为 $$O(log⁡n)$$。

#### 6. B树（B-Tree）

- **定义**：一种自平衡的多路搜索树，节点可以有多个子节点和多个关键字。
- **应用**：数据库和文件系统的索引。
- **特点**：查找、插入、删除操作的时间复杂度为 $$O(log⁡n)$$，适合存储大量数据。

#### 7. B+树（B+ Tree）

- **定义**：B树的变体，所有数据都存储在叶子节点，叶子节点通过链表连接。
- **应用**：数据库和文件系统的索引。
- **特点**：提高了区间查询的效率，所有叶子节点形成一个有序链表，便于顺序访问。

#### 8. 线段树（Segment Tree）

- **定义**：用于存储区间或线段信息的树结构。
- **应用**：动态范围查询，如区间和查询、区间最小值查询。
- **特点**：构建时间和查询时间复杂度为 $$O(nlog⁡n)$$。

#### 9. 树状数组（Fenwick Tree or Binary Indexed Tree, BIT）

- **定义**：用于高效计算前缀和的树状结构。
- **应用**：动态求和问题，如频率计数、区间求和。
- **特点**：更新和查询操作的时间复杂度为 $$O(log⁡n)$$。

#### 10. Trie树（前缀树，Trie）

- **定义**：一种多叉树，用于存储字符串的前缀信息。
- **应用**：字符串查找、自动补全、词频统计。
- **特点**：查找、插入、删除操作的时间复杂度为 $$O(m)$$，其中 $$m$$ 是字符串的长度。

#### 11. 并查集（Disjoint Set Union, DSU）

- **定义**：一种树状数据结构，用于处理不相交集合的合并和查询。
- **应用**：连通性问题、最小生成树算法（Kruskal）。
- **特点**：路径压缩和按秩合并使得操作的均摊时间复杂度接近 $$O(1)$$。

#### 12. AVL树

​	二叉搜索树中，如果每一个子节点都小于等于根节点，那么会导致查找效率大大降低，这也就是我们使用平衡二叉搜索树的原因。$$AVL$$树的平衡因子（左子树和右子树高度之差）绝对值小于1，因此可以大大加快查找效率。

#### 13. 哈夫曼编码树

**哈夫曼编码树**（Huffman Coding Tree）是一种用于无损数据压缩的最优二叉树。它在信息编码过程中，以最小的平均编码长度实现对字符的编码。

##### 关键概念

1. **字符频率**：哈夫曼编码基于字符在数据中出现的频率。频率越高的字符使用的编码长度越短，从而减少了整体编码长度。
2. **二叉树**：哈夫曼编码树是一棵二叉树，每个叶子节点代表一个字符及其对应的编码。
3. **WPL**：把每次合并出的权值求和得到的结果

##### 构建哈夫曼编码树的步骤

1. **统计频率**：计算每个字符在数据中出现的频率。
2. **构建节点**：为每个字符创建一个节点，每个节点包含字符和其频率。
3. **构建优先队列**：将所有节点放入优先队列（最小堆），按频率排序。
4. **合并节点**：从优先队列中取出两个频率最小的节点，创建一个新的父节点，其频率为两个子节点频率之和。将新节点放回优先队列。
5. **重复步骤4**：直到优先队列中只剩一个节点，该节点即为哈夫曼编码树的根节点。

##### 生成哈夫曼编码

在构建好的哈夫曼编码树上，遍历树从根节点到每个叶子节点，左子节点代表二进制编码的"0"，右子节点代表"1"。每个叶子节点的路径即为对应字符的哈夫曼编码。

##### 优点

- **压缩效率高**：哈夫曼编码能够根据字符频率动态调整编码长度，频率高的字符使用较短编码，从而实现高效压缩。
- **无损压缩**：能够无损地恢复原始数据。

##### 缺点

- **依赖字符频率**：需要预先统计字符频率，对于实时数据流可能不适用。
- **额外存储开销**：需要存储哈夫曼树或编码表，增加一定的存储开销。

## Part 5：图最后一个部分！

### 一、定义

略

### 二、图的表示方法（图的存储）

#### 1、直接存边

**方法**：

​	使用一个数组来存边，数组中的每个元素都包含一条边的起点与终点（带边权的图还包含边权）。（或者使用多个数组分别存起点，终点和边权）

参考代码：

```python
class Edge:
    def __init__(self, u=0, v=0):
        self.u = u
        self.v = v

n, m = map(int, input().split())
graph = [Edge() for _ in range(m)]
vis = [False for _ in range(n)]
for i in range(m):
    graph[i].u, graph[i].v = map(int, input().split())

def find_edge(a, b):
    for k in range(m):
        if graph[k].u == a and graph[k].v == b:
            return True
    return False

def dfs(p):
    if vis[p]:
        return
    vis[p] = True
    for j in range(m):
        if graph[j].u == p:
            dfs(graph[j].v)

```

**复杂度**：

​	查询是否存在某条边：$$O(m)$$。

​	遍历一个点的所有出边：$$O(m)$$。

​	遍历整张图：$$O(nm)$$。

​	空间复杂度：$$O(m)$$。

**应用**：

​	由于直接存边的遍历效率低下，一般不用于遍历图。

​	在$$Kruskal$$算法中，由于需要将边按边权排序，需要直接存边。

​	在有的题目中，需要多次建图（如建一遍原图，建一遍反图），此时既可以使用多个其它数据结构来同时存储多张图，也可以将边直接存下来，需要重新建图时利用直接存下的边来建图。



#### 2、邻接矩阵

**方法**：

​	使用一个二维数组$$graph$$来存边，其中$$graph[u][v]$$为$$1$$表示存在$$u$$到$$v$$的边，为$$0$$表示不存在。如果是带边权的图，可以在$$graph[u][v]$$中存储$$u$$到$$v$$的边的边权。

**复杂度**：

​	查询是否存在某条边：$$O(1)$$。

​	遍历一个点的所有出边：$$O(n)$$。

​	遍历整张图：$$O(n^2)$$。

​	空间复杂度：$$O(n^2)$$。

**应用**：

​	邻接矩阵只适用于没有重边（或重边可以忽略）的情况。

​	其最显著的优点是可以$$O(1)$$查询一条边是否存在。

​	由于邻接矩阵在稀疏图上效率很低（尤其是在点数较多的图上，空间无法承受），所以一般只会在稠密图上使用邻接矩阵。



#### 3、邻接表

**方法**：

​	为了实现稀疏连接的图，更高效的方式是使用邻接表。在邻接表实现中，我们为图对象的所有顶点保存一个主列表，同时为每一个顶点对象都维护一个列表，其中记录了与它相连的顶点。

**复杂度**：

​	查询是否存在$$u$$到$$v$$的边：$$O(d^+(v))$$。（若排序，二分查找可以降低复杂度）

​	遍历点$$u$$的所有出边：$$O(d^+(v))$$。

​	遍历整张图：$$O(n+m)$$。

​	空间复杂度：$$O(m)$$。

**应用**：

​	存各种图都很适合，除非有特殊需求（如需要快速查询一条边是否存在，且点数较少，可以使用邻接矩阵）。

​	尤其适用于需要对一个点的所有出边进行排序的场合。

#### 4、链式前向星

​	如果说邻接表是不好写但效率好，邻接矩阵是好写但效率低的话，前向星就是一个相对中庸的数据结构。前向星固然好写，但效率并不高。而在优化为链式前向星后，效率也得到了较大的提升。虽然说，世界上对链式前向星的使用并不是很广泛，但在不愿意写复杂的邻接表的情况下，链式前向星也是一个很优秀的数据结构。

**方法**：

​	本质就是用链表实现的邻接表，但邻接表存的是点，而链式前向星存的是边。

参考代码：

```python
class Edge:
    def __init__(self, to, w, nxt):
        self.to = to
        self.w = w
        self.next = nxt

n, m = map(int, input().split())
edge = [Edge(0, 0, 0) for _ in range(m)]
head = [-1 for _ in range(n+1)]
cnt = 0
for _ in range(m):
    u, v, w = map(int, input().split())
    edge[cnt].to = v
    edge[cnt].w = w
    edge[cnt].next = head[u]
    head[u] = cnt
    cnt += 1
vis = [False for _ in range(n+1)]

def find_edge(a, b):
    i = head[a]
    while i != -1:
        if edge[i].to == b:
            return True
        i = edge[i].next
    return False

def dfs(p):
    if vis[p]:
        return
    vis[p] = True
    i = head[u]
    while i != -1:
        dfs(edge[i].to)
        i = edge[i].next

```

**复杂度**：

​	查询是否存在$$u$$到$$v$$的边：$$O(d^+(u))$$。

​	遍历点$$u$$的所有出边：$$O(d^+(u))$$。

​	遍历整张图：$$O(n+m)$$。

​	空间复杂度：$$O(m)$$。

**应用**：

​	存各种图都很适合，但不能快速查询一条边是否存在，也不能方便地对一个点的出边进行排序。

​	优点是边是带编号的，有时会非常有用，而且如果$$cnt$$的初始值为奇数，存双向边时`i ^ 1`即是`i`的反边（常用于网络流）。

### 三、图的遍历

#### $$\mathrm{I}.$$​ 广度优先搜索

**BFS**使用**队列**实现，适用于逐层遍历、寻找最短路径等场景。

#### $$\mathrm{II}.$$​ 深度优先搜索

**DFS**使用**栈**或**递归**实现，适用于深度优先遍历、路径查找、拓扑排序等场景。

### 四、拓扑排序

**拓扑排序**（Topological Sorting）是用于有向无环图（DAG, Directed Acyclic Graph）的排序方法，它将图中的所有顶点排成一个线性序列，使得对于每一条有向边(u, v)，顶点u在顶点v之前。拓扑排序广泛应用于任务调度、编译程序的语法分析、课程安排等领域。

##### 关键特点

1. **应用对象**：只适用于有向无环图（DAG）。
2. **排序结果**：输出一个线性序列，使得每条边(u, v)都满足u在v之前。
3. **唯一性**：一个图可能有多个拓扑排序结果，但所有结果都符合上述特性。

##### 实现方法

1. Kahn算法：
   - 维护一个入度为0的顶点集合。
   - 每次选择一个入度为0的顶点，将其输出到排序结果中，并从图中删除该顶点和所有以它为起点的边。
   - 更新受影响顶点的入度，并将新的入度为0的顶点加入集合。
2. 深度优先搜索（DFS）：
   - 对图进行DFS，在每次访问完成一个顶点时，将其放入栈中。
   - 最终从栈顶到栈底就是拓扑排序的结果。

### 五、生成树

##### 生成树

**生成树**（Spanning Tree）是给定无向图的一棵子树，它包含图中所有的顶点，并且是一个无环连通图。最小生成树（MST, Minimum Spanning Tree）是生成树的一种，它使得树中所有边的权值之和最小。

##### 关键特点

1. **应用对象**：适用于无向图。
2. **树的性质**：包含图中的所有顶点，且边数为顶点数减一，形成一个无环连通子图。
3. **最小生成树**：选择边权和最小的生成树。

##### 生成树算法

1. **Prim算法**：
   - 从任意一个顶点开始，逐步扩展生成树，每次选择与生成树相连且权值最小的边加入生成树。
2. **Kruskal算法**：
   - 将所有边按权值排序，依次选择权值最小的边，若该边不形成环则加入生成树，直到树包含所有顶点。
